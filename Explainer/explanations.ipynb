{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we work with look-up tables that were created for ยง4.2 to compute Shapley values for the four CatBoost models from ยง4.1 which were trained on public datasets. A sample of size 100 from the test set is provided in each case, and is available in the folder \"Samples\". \n",
    "\n",
    "The look-up tables are created via a proprietary code of Discover Financial Services which is a fast implementation of Algorithm 3.12. These are located in folders \"game_values_loc_1\", \"game_values_loc_2\" etc. where  there is a .csv file for each tree of the ensemble under consideration containing all Shapley values arising from that tree. The rows are indexed by the leaves of the oblivious tree and the columns capture the features on which the tree splits. The non-relaizable leaves corresponding to vacuous regions are excluded. The .json file in each folder relates the local enumeration of features appearing in a tree to their global index in the training data. \n",
    "\n",
    "We verify these precomputed Shapley values through checking the \n",
    "[efficiency axiom](https://christophm.github.io/interpretable-ml-book/shapley.html#the-shapley-value-in-detail): Choosing a tree from one of the four ensembles randomly, for each data sample, the difference between tree's output (i.e. the leaf value) and the sum of Shapley values associated with the corresponding leaf is always a constant -- it should be equal to the avarage of outputs of that tree over the whole training data:\n",
    "\n",
    "$$\\sum_i\\varphi_i[g](\\mathbf{x})=g(\\mathbf{x})-\\mathbb{E}[g]\\quad \\forall\\mathbf{x}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-05T20:41:57.960223Z",
     "iopub.status.busy": "2023-08-05T20:41:57.959828Z",
     "iopub.status.idle": "2023-08-05T20:42:08.928501Z",
     "shell.execute_reply": "2023-08-05T20:42:08.927896Z",
     "shell.execute_reply.started": "2023-08-05T20:41:57.960153Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import glob\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the CatBoost model and the corresponding data sample. Only experiment_number should be declared (a number between 1 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T21:31:39.030352Z",
     "iopub.status.busy": "2023-08-04T21:31:39.029998Z",
     "iopub.status.idle": "2023-08-04T21:31:39.057723Z",
     "shell.execute_reply": "2023-08-04T21:31:39.057143Z",
     "shell.execute_reply.started": "2023-08-04T21:31:39.030328Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider the CatBoost ensemble from experiment 1 which has 300 trees.\n"
     ]
    }
   ],
   "source": [
    "experiment_number=4\n",
    "\n",
    "if experiment_number==1 or experiment_number==2:\n",
    "    model_type='Regressor'\n",
    "elif experiment_number==3 or experiment_number==4:\n",
    "    model_type='Classifier'\n",
    "else:\n",
    "    raise ValueError('experiment_number should be 1,2,3 or 4.')\n",
    "    \n",
    "sample_path='./Samples/Sample_'+str(experiment_number)+'.csv'\n",
    "sample=pd.read_csv(sample_path)\n",
    "n_samples=sample.shape[0]\n",
    "\n",
    "model_path='./Models/'+model_type+'_CatBoost_'+str(experiment_number)\n",
    "model_cat=pickle.load(open(model_path,'rb'))\n",
    "\n",
    "local_shapley_folder_path='./game_value_loc_'+str(experiment_number)\n",
    "n_trees=len(glob.glob1(local_shapley_folder_path,'*.csv'))\n",
    "print(f'We consider the CatBoost ensemble from experiment {experiment_number} which has {n_trees} trees.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below computes and stores the average of outputs of each tree from the ensemble over the training data. The function retrieve_catboost from the Retrieve_splits notebook is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T21:31:41.645692Z",
     "iopub.status.busy": "2023-08-04T21:31:41.645409Z",
     "iopub.status.idle": "2023-08-04T21:31:42.813652Z",
     "shell.execute_reply": "2023-08-04T21:31:42.813033Z",
     "shell.execute_reply.started": "2023-08-04T21:31:41.645670Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tree_averages(model_cat):\n",
    "    \n",
    "    #Dumping the CatBoost model as a dictionary. \n",
    "    if (not isinstance(model_cat,CatBoostClassifier)) and (not isinstance(model_cat,CatBoostRegressor)):\n",
    "        raise TypeError('The input should be a CatBoost classifier or regressor.')\n",
    "    model_cat.save_model('temp',format='json')\n",
    "    model=open('temp')\n",
    "    dictionary_catboost=json.load(model)\n",
    "    os.remove('temp')\n",
    "    \n",
    "    #The average associated with a tree is the sum of products region['value']*region['probability'] over all leaves.\n",
    "    averages=[]\n",
    "    for tree_structure in dictionary_catboost['oblivious_trees']:\n",
    "        regions=retrieve_catboost(tree_structure)['regions']\n",
    "        probabilities=[]\n",
    "        values=[]\n",
    "        #Extracting the value and the probability associated with each leaf (or equivalently, with the corresponding region.)\n",
    "        for region in regions:                     \n",
    "            probabilities+=[region['probability']]\n",
    "            values+=[region['value']]\n",
    "        average=np.dot(np.asarray(probabilities),np.asarray(values))\n",
    "        averages+=[average]\n",
    "    \n",
    "    return averages\n",
    "##############################################################################        \n",
    "#The function retrieve_catboost from the Retrieve_splits notebook.\n",
    "\n",
    "def retrieve_catboost(tree_structure):\n",
    "    #Initializing the dictionary:\n",
    "    info={}\n",
    "    \n",
    "    #The first two keys are easy:\n",
    "    info['depth']=len(tree_structure['splits'])\n",
    "    info['n_leaves']=2**info['depth']\n",
    "    \n",
    "    #Initializing the next two keys:\n",
    "    info['splits']=[]\n",
    "    info['distinct_feature_indx']=[]\n",
    "    \n",
    "    for split in tree_structure['splits']:               #Each element of tree_structure['splits'] describes a splitting that takes place across an entire level.\n",
    "        if split['float_feature_index'] not in info['distinct_feature_indx']:\n",
    "            info['distinct_feature_indx']+=[split['float_feature_index']]\n",
    "        info['splits']+=[(split['float_feature_index'],split['border'])]\n",
    "        \n",
    "    \n",
    "    #It remains to compute info['region'], a list comprised of one dictionary per region. \n",
    "    #Initializing:\n",
    "    info['regions']=[]\n",
    "    \n",
    "    for i in range(2**info['depth']):\n",
    "        #Constructing the dictionary describing this region:\n",
    "        region={}\n",
    "        region['value']=tree_structure['leaf_values'][i]\n",
    "        region['weight']=tree_structure['leaf_weights'][i]\n",
    "        \n",
    "        #Initializing the keys that describe bounds for each feature.\n",
    "        for feature_index in info['distinct_feature_indx']:\n",
    "            region[feature_index]=[-float('inf'),float('inf')]\n",
    "            \n",
    "        expansion='{0:b}'.format(i)                          #The binary expansion of i which is the index of the leaf/region under consideration.\n",
    "        while len(expansion)<info['depth']:                  #(An integer from [0,2**depth-1], we want len(expansion)=depth.) \n",
    "            expansion='0'+expansion\n",
    "                                                             \n",
    "        for j in range(info['depth']):                       #The leftmost characters of the expansion are determined by top splits near \n",
    "            feature_index=info['splits'][-j-1][0]            #the root which are encoded by the rightmost entries of info['splits'].         \n",
    "            threshold=info['splits'][-j-1][1]                #(Keep in mind that splits closer to the root appear at the end of tree_structure['splits']).\n",
    "            \n",
    "            if expansion[j]=='0':                            #Meaning we go to the left since feature_value<threshold.\n",
    "                region[feature_index]=modify_interval(region[feature_index],threshold,'upper')\n",
    "            else:                                            #Meaning we go to the left since feature_value>threshold.\n",
    "                region[feature_index]=modify_interval(region[feature_index],threshold,'lower')\n",
    "        \n",
    "        #Adding the dictionary constructed for this region to info['regions'].\n",
    "        info['regions']+=[region]\n",
    "        \n",
    "        \n",
    "    #Adding a key for porbability to each dictionary from info['regions']\n",
    "    total_weight=0\n",
    "    for region in info['regions']:\n",
    "        total_weight+=region['weight']\n",
    "    for region in info['regions']:\n",
    "        region['probability']=region['weight']/total_weight\n",
    "    \n",
    "    return info\n",
    "\n",
    "############\n",
    "#An auxiliary function \n",
    "\n",
    "def modify_interval(interval,bound,kind):\n",
    "    if interval==None:                      #Nothing to modify if the interval is empty to begin with. \n",
    "        return None\n",
    "    if kind=='upper':\n",
    "        if interval[0]>=bound:\n",
    "            return None\n",
    "        else:\n",
    "            interval[1]=min(interval[1],bound)\n",
    "    else:\n",
    "        if interval[1]<=bound:\n",
    "            return None\n",
    "        else:\n",
    "            interval[0]=max(interval[0],bound)\n",
    "    return interval            \n",
    "\n",
    "##############################################################################\n",
    "#Saving the averages for all trees in the ensemble\n",
    "averages=tree_averages(model_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-04T21:31:44.031890Z",
     "iopub.status.busy": "2023-08-04T21:31:44.031515Z",
     "iopub.status.idle": "2023-08-04T21:31:44.129438Z",
     "shell.execute_reply": "2023-08-04T21:31:44.128890Z",
     "shell.execute_reply.started": "2023-08-04T21:31:44.031866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tree of index 120 was chosen randomly from the CatBoost ensemble.\n",
      "The average of its outputs over the training data is -0.0022147689080705183.\n",
      "\n",
      "Verifying the efficiency axiom: the sum of local Shapley values minus the output should be the same for all 100 sample data points; this difference always coincides with the average ouput of the tree.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477,\n",
       "       -0.00221477, -0.00221477, -0.00221477, -0.00221477, -0.00221477])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We pick a random tree. Since leaves corresponding to degenerate regions are not considered \n",
    "#in look-up tables, we only consider tables with (number of rows)=2**(number of columns),\n",
    "#that is, trees without repeated features. \n",
    "#For such trees, the internal enumeration of leaves matches the order of rows. \n",
    "\n",
    "while True:\n",
    "    tree_index=random.randint(0,n_trees-1)\n",
    "    local_shapley=pd.read_csv(local_shapley_folder_path+'/game_value_tree_'+str(tree_index)+'.csv',\n",
    "                         header=None)\n",
    "    if local_shapley.shape[0]==2**(local_shapley.shape[1]):\n",
    "        break\n",
    "print(f'The tree of index {tree_index} was chosen randomly from the CatBoost ensemble.')\n",
    "print(f'The average of its outputs over the training data is {averages[tree_index]}.')\n",
    "        \n",
    "#The outputs of the chosen tree. These are leaf values which for classifiers are logit probability.         \n",
    "outputs=model_cat.predict(sample,prediction_type='RawFormulaVal',\n",
    "                          ntree_start=tree_index,ntree_end=tree_index+1)\n",
    "        \n",
    "\n",
    "#Determining leaves of the tree at which sample points land:\n",
    "leaf_indices=model_cat.calc_leaf_indexes(sample,ntree_start=tree_index,ntree_end=tree_index+1).reshape(n_samples)\n",
    "\n",
    "#Adding the sum of rows to the table of shapley values\n",
    "local_shapley['sum']=local_shapley.sum(axis=1)\n",
    "\n",
    "#Subtracting the sum of Shapley values at the leaf corresponding to a sample point from the leaf value:\n",
    "difference=outputs-np.asarray(local_shapley['sum'][leaf_indices].to_list())\n",
    "print('\\nVerifying the efficiency axiom: the sum of local Shapley values minus the output should be the same for all 100 sample data points; this difference always coincides with the average ouput of the tree.')\n",
    "difference"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
